import os
from dotenv import load_dotenv
from llama_parse import LlamaParse
from openai import OpenAI
import nest_asyncio

# 1. åŠ è½½ç¯å¢ƒå˜é‡ (è‡ªåŠ¨è¯»å– .env)
load_dotenv()

# è§£å†³ Jupyter/Streamlit å¼‚æ­¥å¾ªç¯å†²çªçš„è¡¥ä¸
nest_asyncio.apply()

def get_api_key(key_name):
    """å®‰å…¨åœ°è·å– API Key"""
    key = os.getenv(key_name)
    if not key:
        raise ValueError(f"âŒ æ‰¾ä¸åˆ° {key_name}ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶ï¼")
    return key

def parse_pdf(file_path):
    """
    ã€API è°ƒç”¨ 1ã€‘ä½¿ç”¨ LlamaParse è§£æ PDF
    """
    print(f"[Log] æ­£åœ¨è°ƒç”¨ LlamaParse è§£æ: {file_path}...")
    
    parser = LlamaParse(
        api_key=get_api_key("LLAMA_CLOUD_API_KEY"),
        result_type="markdown", 
        verbose=True,
        language="en", 
    )
    
    # è·å–æ–‡æ¡£åˆ—è¡¨ (è¿™é‡Œå¯èƒ½åŒ…å«å¤šä¸ª Document å¯¹è±¡ï¼Œä¾‹å¦‚ä¸€é¡µä¸€ä¸ª)
    documents = parser.load_data(file_path)
    
    if not documents:
        raise ValueError("LlamaParse è§£æå¤±è´¥ï¼Œè¿”å›äº†ç©ºç»“æœã€‚")
    
    # --- ğŸ”¥ æ ¸å¿ƒä¿®å¤å¼€å§‹ ---
    # æ—§ä»£ç ï¼šreturn documents[0].text  <-- åªå–äº†ç¬¬ä¸€é¡µ
    
    # æ–°ä»£ç ï¼šä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼ŒæŠŠæ‰€æœ‰é¡µé¢çš„ text æå–å‡ºæ¥ï¼Œç”¨æ¢è¡Œç¬¦æ‹¼åœ¨ä¸€èµ·
    full_text = "\n\n".join([doc.text for doc in documents])
    # --- æ ¸å¿ƒä¿®å¤ç»“æŸ ---

    print(f"[Log] è§£ææˆåŠŸï¼Œå…±æå–äº† {len(full_text)} ä¸ªå­—ç¬¦ã€‚")
    return full_text

def chat_with_ai(context, question):
    """
    ã€API è°ƒç”¨ 2ã€‘è°ƒç”¨ DeepSeek å›ç­”é—®é¢˜
    è¾“å…¥ï¼šæ–‡ç« å†…å®¹(context), ç”¨æˆ·é—®é¢˜(question)
    è¾“å‡ºï¼šAI çš„å›ç­”å­—ç¬¦ä¸²
    """
    client = OpenAI(
        api_key=get_api_key("DEEPSEEK_API_KEY"),
        base_url=os.getenv("DEEPSEEK_BASE_URL")
    )

    # æç¤ºè¯å·¥ç¨‹ (Prompt Engineering)
    # æˆ‘ä»¬æŠŠä» PDF è§£æå‡ºæ¥çš„å¾ˆé•¿çš„æ–‡æœ¬ï¼Œå¡è¿› 'system' æˆ–è€… 'user' çš„ä¸Šä¸‹æ–‡ä¸­
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„è®ºæ–‡å†…å®¹å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“ï¼Œå°±è¯´ä¸çŸ¥é“ã€‚"
    
    user_prompt = f"""
    ã€è®ºæ–‡å†…å®¹å¼€å§‹ã€‘
    {context[:30000]} 
    ã€è®ºæ–‡å†…å®¹ç»“æŸã€‘
    
    (æ³¨æ„ï¼šä¸ºäº†é˜²æ­¢è¶…é•¿ï¼Œä¸Šé¢åªæˆªå–äº†å‰30000å­—ç¬¦ã€‚å®é™…ç”Ÿäº§ä¸­éœ€è¦ç”¨ RAG åˆ‡ç‰‡ï¼Œä½†åœ¨ Week1 æˆ‘ä»¬å…ˆåšç®€å•çš„ä¸Šä¸‹æ–‡å¡«å……)

    ç”¨æˆ·é—®é¢˜ï¼š{question}
    """

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.1 # å­¦æœ¯é—®é¢˜éœ€è¦ä¸¥è°¨ï¼Œæ¸©åº¦è°ƒä½
    )
    
    return response.choices[0].message.content